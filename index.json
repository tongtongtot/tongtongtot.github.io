[{"content":"\u003ch4 id=\"this-is-my-introduction-video-i-hope-you-will-enjoy-it\"\u003eThis is my introduction video. I hope you will enjoy it!\u003c/h4\u003e\n\u003ciframe height=500 width=888 src=\"/images/Introduction.mp4\"\u003e\u003c/iframe\u003e\n","description":"Hi there, I'm Tang Zijia. I am really glad to share my story with you. This is my introduction video. I hope you will enjoy it!","image":"/images/introduction.jpeg","permalink":"https://tongtongtot.github.io/blogs/introduction/","title":"Introduction video"},{"content":"\u003ch2 id=\"scperb-single-cell-perturbation-via-style-transfer-based-variational-autoencoder\"\u003eSCPERB: SINGLE-CELL PERTURBATION VIA STYLE TRANSFER-BASED VARIATIONAL AUTOENCODER\u003c/h2\u003e\n\u003cp\u003eTraditional methods for obtaining cellular responses after perturbation are usually labor-intensive and costly, especially when working with rare cells or under severe experimental conditions. Therefore, accurate prediction of cellular responses to perturbations is of great importance in computational biology. To address this problem, some methodologies have been previously developed, including graph-based approaches, vector arithmetic, and neural networks. However, these methods either mix the perturbation-related variances with the cell-type-specific patterns or implicitly distinguish them within black-box models. In this work, we introduce a novel framework, scPerb, to explicitly extract the perturbation-related variances and transfer them from perturbed data to control data. scPerb adopts the style transfer strategy by incorporating a style encoder into the architecture of a variational autoencoder. Such style encoder accounts for the differences in the latent representations between control cells and perturbed cells, which allows scPerb to accurately predict the gene expression data of perturbed cells. Through the comparisons with existing methods, scPerb presents improved performance and higher accuracy in predicting cellular responses to perturbations. Specifically, scPerb not only outperforms other methods across multiple datasets, but also achieves superior R2 values of 0.98, 0.98, and 0.96 on three benchmarking datasets.\u003c/p\u003e\n\u003ch2 id=\"highlights\"\u003eHighlights\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003escPerb merged the concept of style transfer and VAE, resulting in incredible accuracy.\u003c/li\u003e\n\u003cli\u003escPerb outperforms all the existing models of single-cell perturbation prediction\u003c/li\u003e\n\u003cli\u003escPerb is developed and tailored for single-cell perturbation prediction and provided as a ready-to-use open-source software, demonstrating high accuracy and robust performance over existing methods.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"faq\"\u003eFAQ\u003c/h2\u003e\n\u003ch4 id=\"how-can-i-install-scperb\"\u003e\u003cstrong\u003eHow can I install scPerb?\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eYou can download scPerb from our GitHub link:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egit clone https://github.com/tongtongtot/scperb.git\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003escPerb is built based on PyTorch, tested in Ubuntu 18.04, CUDA environment (cuda 11.6).\u003c/p\u003e\n\u003cp\u003eThe requirement packages include:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eanndata==0.10.3\nmatplotlib==3.7.2\nnumpy==1.24.3\npandas==2.0.3\nscanpy==1.9.6\nscipy==1.11.1\nseaborn==0.12.2\ntorch==2.1.0\ntorchaudio==2.1.0\ntorchvision==0.16.0\ntqdm==4.65.0\nwget==3.2\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eor you can also use the following scripts:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epip install -r requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"i-want-to-try-the-pbmc-demo-can-i-run-scperb-in-one-command-line\"\u003e\u003cstrong\u003eI want to try the PBMC demo, can I run scPerb in one command line?\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eYou can use the following commands:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epython3 scperb.py\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eor please refer to our training tutorial \u003ca href=\"https://github.com/tongtongtot/scperb-tutorial\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"do-i-need-a-gpu-for-running-scperb\"\u003e\u003cstrong\u003eDo I need a GPU for running scPerb?\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003escPerb can run on a standard laptop without GPU. For computational efficiency, we recommend you use a GPU. scPerb could detect whether there is an available GPU or not, so do not worry about this.\u003c/p\u003e\n\u003ch4 id=\"can-i-generate-my-configuration-file-using-the-command-line\"\u003e\u003cstrong\u003eCan I generate my configuration file using the command line?\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eYou can change the default settings in the option.py, while the usage and description are written inside the document.\u003c/p\u003e\n\u003ch4 id=\"link\"\u003e\u003cstrong\u003eLink?\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"http://yau-awards.com/uploads/file/20231031/20231031150434_30639.pdf\"\u003eThe paper is under this link.\u003c/a\u003e\u003c/p\u003e\n","description":"","image":"/images/projects/scPerb.jpeg","permalink":"https://tongtongtot.github.io/blogs/scperb/","title":"SCPERB"},{"content":"\u003cp\u003eI joined the Song Lab at Wake Forest University School of Medicine as a high school intern and was fortunate to collaborate with \u003ca href=\"https://qsong-github.github.io/SongLab/\"\u003eProfessor Song\u003c/a\u003e on two research projects. Excited by the application of neural networks in medicine and inspired by Prof. Song’s previous work, I initiated two studies detailed below. I spent, on average, an hour every day during the school year and 8 hours a day during summer on research. Prof. Song primarily specializes in the field of medicine; therefore, I undertook the computer science aspects of my research independently.\u003c/p\u003e\n\u003ch3 id=\"pinet-privileged-information-improve-the-interpretability-and-generalization-of-structural-mri-in-alzheimers-diseasehttpsdlacmorgdoi10114535843713613000\"\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/3584371.3613000\"\u003ePINet: Privileged Information Improve the Interpretability and generalization of structural MRI in Alzheimer\u0026rsquo;s Disease\u003c/a\u003e\u003c/h3\u003e\n\u003ch4 id=\"introduction\"\u003eIntroduction\u003c/h4\u003e\n\u003cp\u003ePINet is a neural network model devised to detect Alzheimer’s disease from MRI images even before symptoms occur. The research idea focusing on early Alzheimer’s detection originated from my grandpa’s diagnosis of late-stage Alzheimer’s. Before his diagnosis, my parents and I had attributed his occasional forgetfulness and throwing tantrums to being old and angry over our lack of company, only to realize his condition when it was already too late for any form of intervention. We witnessed his condition develop into dementia and his memory and life were slowly taken away from him. I devoted time to researching the early diagnosis and intervention of Alzheimer’s and realized the potential for neural networks to play a pivotal role.\u003c/p\u003e\n\u003cp\u003eThis paper was accepted (acceptance rate 29%) by the ACM-BCB conference as a rapid-fire paper (top 25% of accepted papers)\u003c/p\u003e\n\u003ch4 id=\"abstract\"\u003eAbstract\u003c/h4\u003e\n\u003cp\u003eThe irreversible and progressive atrophy of Alzheimer\u0026rsquo;s Disease resulted in a continuous decline in thinking and behavioral skills. To date, CNN classifiers have been widely applied to assist in the early diagnosis of AD and its associated abnormal structures. However, most existing black-box CNN classifiers relied heavily on limited MRI scans and used little domain knowledge from previous clinical findings. In this study, we proposed a framework, \u003cem\u003ePINet\u003c/em\u003e, to consider previous domain knowledge as \u003cem\u003ePrivileged Information (PI)\u003c/em\u003e, and peak into the black-box in the prediction process. \u003cem\u003ePINet\u003c/em\u003e uses a Transformer-like fusion module \u003cem\u003ePrivileged Information Fusion (PIF)\u003c/em\u003e to iteratively calculate the correlation of the features between image features and \u003cem\u003ePI\u003c/em\u003e features, and project the features into a latent space for classification. \u003cem\u003ePINet\u003c/em\u003e is suitable for neuro-imaging tasks and we demonstrated its application in Alzheimer\u0026rsquo;s Disease using structural MRI scans from the ADNI dataset. The F1-score showed that \u003cem\u003ePINet\u003c/em\u003e is more robust in transferring to a new dataset, with an approximately 2% drop (from 0.9471 to 0.9231), while the baseline CNN methods had a 29% drop (from 0.8679 to 0.6154). Our best model was trained under the guidance of 12 selected ROIs, majoring in the structures of the Temporal Lobe and Occipital Lobe. In summary, \u003cem\u003ePINet\u003c/em\u003e incorporates domain knowledge as \u003cem\u003ePI\u003c/em\u003e to train the CNN model, and the selected \u003cem\u003ePI\u003c/em\u003e introduces both interpretability and generalization ability to the black box CNN classifiers.\u003c/p\u003e\n\u003ch4 id=\"overview\"\u003eOverview\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/graph_pinet.png\" alt=\"img\"\u003e\u003c/p\u003e\n\u003ch4 id=\"results\"\u003eResults\u003c/h4\u003e\n\u003cimg src=\"/images/projects/pinet_results.png\" width = 75% align=\"middle\"/\u003e\n\u003cp\u003eThe accuracy and F1 score of our model is 96% (at the third row)\u003c/p\u003e\n\u003cimg src=\"/images/projects/Pinet-result2.png\" width = 75% align=\"middle\"/\u003e\n\u003cp\u003eThe visualization result of our model.\u003c/p\u003e\n\u003ch3 id=\"scperb-single-cell-perturbation-via-style-transfer-based-variational-autoencoderhttpwwwyau-awardscomuploadsfile2023103120231031150434_30639pdf\"\u003e\u003ca href=\"http://www.yau-awards.com/uploads/file/20231031/20231031150434_30639.pdf\"\u003escPerb: single-cell perturbation via style transfer-based variational autoencoder\u003c/a\u003e\u003c/h3\u003e\n\u003ch4 id=\"introduction-1\"\u003eIntroduction\u003c/h4\u003e\n\u003cp\u003eHoping to leverage my technical skills to analyze single-cell RNA-seq, I proposed a study to generate single-cell perturbation using generative models. At the current stage, it’s challenging to generate sufficient gene expressions in response to different drugs, doses, and treatments for it’s too labor-intensive and costly: computational tools can fill the gap. Lacking a strong background in bioinformatics, I spent a month reading 50+ papers to familiarize myself with the topic. I first learned by rebuilding one of our top benchmarking models published in Nature Methods, scGen, step by step. scGen is already a top-notch model that’s difficult to improve. However, after rewriting the entire TensorFlow codes into PyTorch-style code and successfully regenerating results, I thought of replacing the fixed linear transformation matrix used in the model to possibly achieve higher potentials. After spending months trying different methods, from multi-layers, multi-encoders, different loss functions like Weighted MSE or ZINB loss and even tinkering with other high-performing models like GANs or VQVAE, I found a rather simple approach: I added style transfer, commonly used for scenarios like transferring the styles of oil paintings to digital images. I innovatively proposed RNA sequences as “styles,” decoupling style-related and content-based features to better “learn” the difference in distribution, mitigating the limitations posed by “vague” images generated by VAEs, and increasing accuracy significantly. Resulted in an $R^2$ value of 0.99.\u003c/p\u003e\n\u003ch4 id=\"abstract-1\"\u003eAbstract\u003c/h4\u003e\n\u003cp\u003eTraditional methods for obtaining cellular responses after perturbation are usually labor-intensive and costly, especially when working with rare cells or under severe experimental conditions. Therefore, accurate prediction of cellular responses to perturbations is of great importance in computational biology. To address this problem, some methodologies have been previously developed, including graph-based approaches, vector arithmetic, and neural networks. However, these methods either mix the perturbation-related variances with the cell-type-specific patterns or implicitly distinguish them within black-box models. In this work, we introduce a novel framework, scPerb, to explicitly extract the perturbation-related variances and transfer them from control data to perturbed data. scPerb adopts the style transfer strategy by incorporating a style encoder into the architecture of a variational for the differences in the latent representations between control cells and perturbed cells, which allows scPerb to accurately predict the gene expression data of perturbed cells. Through the comparisons with existing methods, scPerb presents improved performance and higher accuracy in predicting cellular responses to perturbations. Specifically, scPerb not only outperforms other methods across multiple datasets but also achieves superior $R^2$ values of 0.98, 0.98, and 0.96 on three benchmarking datasets.\u003c/p\u003e\n\u003ch4 id=\"overview-1\"\u003eOverview\u003c/h4\u003e\n\u003cp\u003e\u003cimg src=\"/images/projects/IMG_00001.jpeg\" alt=\"img\"\u003e\u003c/p\u003e\n\u003ch4 id=\"results-1\"\u003eResults\u003c/h4\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/images/projects/IMG_00001-3.jpeg\" width=41.75%/\u003e\n\u003cimg src=\"/images/projects/IMG_00001-4.jpeg\" width=53.25%/\u003e\n\u003c/figure\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/images/projects/IMG_00001-1.jpeg\" width=46.5%/\u003e\n\u003cimg src=\"/images/projects/IMG_00001-2.jpeg\" width=48.5%/\u003e\n\u003c/figure\u003e","description":"","image":"/images/projects/research.png","permalink":"https://tongtongtot.github.io/blogs/research/","title":"My Research Experience"},{"content":"\u003ch3 id=\"postpartum-depression-care-project\"\u003ePostpartum Depression Care Project\u003c/h3\u003e\n\u003cp\u003eThe \u003cem\u003ePregnant Care free\u003c/em\u003e app is a dedicated platform designed to assist mothers in mitigating the symptoms of postpartum depression. In our app, we hope to build a warm and easy place for you to relax: with places for sharing daily moments, posting emotions, and engaging in forums, you can say whatever you want with other postpartum women or experienced experts. Experts can offer professional guidance through advice posts, enhancing support for regular users. Hope our app could make you feel embraced and guided through your postpartum journey!\u003c/p\u003e\n\u003ch4 id=\"introduction-video\"\u003eIntroduction Video\u003c/h4\u003e\n\u003ciframe height=500 width=888 src=\"/images/projects/PPD-intro.mp4\"\u003e\u003c/iframe\u003e\n\u003ch4 id=\"login-pages\"\u003eLogin pages\u003c/h4\u003e\n\u003cp\u003eThis is the login page of our app.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/images/projects/login.png\" width=300/\u003e\n\u003cimg src=\"/images/projects/register.png\" width=300/\u003e\n\u003c/figure\u003e\n\u003ch4 id=\"daily\"\u003eDaily\u003c/h4\u003e\n\u003cp\u003eIn this page, you could share your ideas with others freely.\u003c/p\u003e\n\u003cp\u003eAnd you could also find others having the same idea with you.\u003c/p\u003e\n\u003cp\u003eIf there is something you do not want to share with strangers.\u003cbr\u003e\n\u003cem\u003e\u003cstrong\u003eNo problem!\u003c/strong\u003e\u003c/em\u003e\u003cbr\u003e\nYou could just share them with your friends.\u003c/p\u003e\n\u003cp\u003eFriends are added by searching their phone numbers.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/images/projects/Daily1.png\" width=300/\u003e\n\u003cimg src=\"/images/projects/Daily.png\" width=300/\u003e\n\u003c/figure\u003e\n\u003ch4 id=\"experts-advice\"\u003eExperts advice\u003c/h4\u003e\n\u003cp\u003eIn this section, you would get advice from experts in this field.\u003c/p\u003e\n\u003cp\u003eJust relax, their suggestion and ideas would help you a lot.\u003c/p\u003e\n\u003cimg src=\"/images/projects/expert.png\" width=450/\u003e\n\u003ch4 id=\"playground\"\u003ePlayground\u003c/h4\u003e\n\u003cp\u003eReading and exercising daily could help you feel better.\u003c/p\u003e\n\u003cp\u003eTry to keep on reading and exercising!\u003c/p\u003e\n\u003cp\u003eYou will feel yourself totally different a week later.\u003c/p\u003e\n\u003cfigure\u003e\n\u003cimg src=\"/images/projects/playground1.png\" width=300/\u003e\n\u003cimg src=\"/images/projects/playground2.png\" width=300/\u003e\n\u003c/figure\u003e\n\u003ch4 id=\"download\"\u003eDownload\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://play.google.com/store/apps/details?id=pregproject.pregproject\u0026amp;pli=1\"\u003eGoogle pay\u003c/a\u003e\u003c/p\u003e\n","description":"","image":"/images/projects/PPD.png","permalink":"https://tongtongtot.github.io/blogs/care-free/","title":"Pregnant Carefree"}]